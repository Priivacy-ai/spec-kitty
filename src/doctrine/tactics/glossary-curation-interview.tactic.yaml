schema_version: "1.0"
id: glossary-curation-interview
name: Glossary Curation Interview
purpose: Systematically expand a living glossary through structured HiC-led curation rounds, ensuring all new terms are clear, well-grouped, and approved before inclusion.
references:
  - name: Kitty Glossary Writing Styleguide
    type: styleguide
    id: kitty-glossary-writing
    when: Consult throughout all definition-writing and rewriting steps to enforce clarity, anti-pattern avoidance, and HiC terminology standards.
steps:
  - title: Inventory source materials
    description: >
      Before extracting terms, catalog every source that may contain domain terminology.
      Sources fall into four categories: (1) Codebase — class names, module names, function names,
      enum values, docstrings, error messages, and test descriptions. (2) Documentation — README files,
      ADRs, specs, plans, data-model files, and user guides. (3) Specifications — feature specs,
      plan.md files, data-model.md files, acceptance criteria, and event storming outputs.
      (4) Communication — issue tracker descriptions, PR discussions, and design review notes.
      Record which sources are available and how to access them before proceeding to extraction.
    examples:
      - "For a 430-commit diff: 8 new Python modules in src/specify_cli/dossier/, 3 feature specs in kitty-specs/, 2 new ADRs in architecture/adrs/."
      - "Source inventory: dossier/ (8 modules), glossary/ (16 modules), constitution/ (11 modules), next/ (3 modules), plus 6 untracked feature spec directories."

  - title: Extract candidate terms
    description: >
      Use one or more extraction methods depending on scale. (A) Manual extraction for small scopes:
      grep for class/function/enum names, scan docstrings and comments for domain nouns.
      (B) Automated extraction for larger scopes: use AST parsing to pull class and function names,
      diff commits to find new symbols, or run structured diffs across branches.
      (C) LLM-assisted extraction for prose-heavy sources: feed spec.md, plan.md, and data-model.md
      content with the prompt "extract domain nouns and noun phrases that represent concepts, not
      generic terms." Combine results from all methods into a single candidate list.
    examples:
      - "AST diff of 430 commits extracted 120 new class/function names; LLM pass over 6 spec.md files added 40 domain noun phrases; combined list before dedup: 160 candidates."
      - "Manual scan of src/specify_cli/dossier/models.py yielded: MissionDossier, ArtifactRef, ArtifactClassEnum, ArtifactKey, ContentHash."

  - title: Filter and deduplicate candidates
    description: >
      Apply strict filters to remove noise from the candidate list.
      Exclude: (1) Generic terms — data, info, manager, handler, helper, utils.
      (2) Framework/library terms — BaseModel, Protocol, dataclass, Enum (unless domain-specific).
      (3) Implementation details — list, dict, array, string, int.
      (4) Already-defined terms — cross-reference against all existing glossary context files,
      the YAML seed file, and historical-terms.md.
      Include: (1) Domain nouns — Mission Dossier, Parity Hash, Constitution Interview.
      (2) Domain processes — Artifact Indexing, Template Resolution, Mission Discovery.
      (3) Domain states — Completeness Status, Parity Drift, Collaboration Mode.
      Record the before/after count to track filtering effectiveness.
    examples:
      - "160 raw candidates → filtered out 38 generic terms, 22 framework terms, 12 implementation details, 44 already-defined → 44 net new candidates."
      - "Rejected 'BaseModel' (Pydantic framework), 'handler' (generic), 'ArtifactClassEnum' (implementation detail — kept 'Artifact Class' as the domain concept instead)."

  - title: Assess quality gates per candidate
    description: >
      Before a candidate enters the curation interview, verify it passes five quality gates:
      (1) Consistent usage — the term appears in more than one place (not a one-off naming accident).
      (2) Clear ownership — a glossary context domain can reasonably own this term.
      (3) Unambiguous definition possible — a domain expert could validate the definition.
      (4) Related to existing terms — the term connects to at least one existing glossary entry.
      (5) Non-obvious — the term adds value that a reader couldn't guess from the word alone.
      Terms that fail multiple gates should be flagged for HiC review rather than silently dropped.
    examples:
      - "'Parity Hash' passes all 5: used in snapshot.py and drift_detector.py (consistent), owned by Dossier context, definable, related to Content Hash, non-obvious to newcomers."
      - "'Baseline Key' scores 0.9 confidence — used in only one module but closely tied to Parity Baseline, so it passes with a note."

  - title: Group candidates by domain fit
    description: Organize candidates into clusters that match existing glossary context files or propose new contexts where coverage gaps are large enough to warrant a dedicated file.
    examples:
      - "14 dossier terms with zero existing coverage warrant a new dossier.md context file."
      - "3 orchestrator API terms fit naturally into the existing orchestration.md context."

  - title: Identify missing foundational terms
    description: Check whether candidate definitions rely on terms (e.g., API, CLI, YAML) that the reader may not know. Add foundational terms to a technology-foundations context so definitions stand on their own.
    examples:
      - "Multiple definitions reference 'YAML' and 'JSON' without explanation — add these to technology-foundations.md."

  - title: Conduct curation rounds with the HiC
    description: Present each group to the Human-in-Charge for review. For each group, the HiC decides whether to accept, tweak, or drop terms. Iterate on definitions until the HiC confirms they are clear.
    examples:
      - "Round 1: Dossier Core (7 terms) — HiC accepts 6 as-is, requests expanded definition for Mission Dossier."
      - "Round 4: HiC requests consistent use of 'Human-in-Charge (HiC)' instead of 'user' across all definitions."

  - title: Rewrite definitions for clarity
    description: Apply the writing styleguide to all definitions. Replace buzzword soup with functional, human-focused language that explains what a term does, not what it is composed of. Test each definition by asking "would a newcomer understand this?"
    examples:
      - "Before: 'Deterministic SHA256 computed from sorted artifact content hashes.' After: 'Checksum computed from all artifact content hashes, sorted to guarantee the same result regardless of scan order. If any artifact changes, the parity hash changes too.'"

  - title: Apply HiC terminology consistently
    description: Replace generic references to 'user' with 'Human-in-Charge (HiC)' in all definitions where the human holds decision authority. This reinforces accountability throughout the glossary.
    examples:
      - "'A structured choice presented to the user' becomes 'A structured choice presented to the Human-in-Charge (HiC) or their delegated agent.'"

  - title: Add all terms as candidate status
    description: New terms enter the glossary as candidates so the upstream HiC retains final promotion authority. Only the HiC can promote a term from candidate to canonical.
    examples:
      - "All 55 terms from the curation session entered as status: candidate."

  - title: Update the domain index
    description: After all terms are written, update the glossary README to reflect new context files, renamed files, and updated domain summaries. Verify all cross-references resolve correctly.
    examples:
      - "Add Dossier, Lexical, and Technology Foundations to the domain index. Update Events & Telemetry reference to System Events."
